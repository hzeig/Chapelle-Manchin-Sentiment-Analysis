dtm[1:10, 1:20]
tdm <- TermDocumentMatrix(ns_corpus)
tdm<- as.matrix(tdm)
tdm[1:100, 1:20]
# Bar Plot
w <- rowSums(tdm)
w <- subset(w, w>=100)
barplot(w, las = 2, col = rainbow(50))
w <- subset(w, w>=150)
w <- subset(w, w>=200)
barplot(w, las = 2, col = rainbow(50))
corpus <- tm_map(corpus, removeWords, c('dave', 'chapelle', 'netflix', 'trans', 'walkout', 'chappelle','just','like'))
ns_corpus <- tm_map(corpus, removeWords, stopwords('english'))
tdm <- TermDocumentMatrix(ns_corpus)
barplot(w, las = 2, col = rainbow(50))
barplot(w, las = 2, col = rainbow(50))
barplot(w, las = 2, col = rainbow(50))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c())
library(wordcloud)
w <- sort(rowSums(tdm),decreasing = TRUE)
library(wordcloud)
w <- sort(rowSums(tdm),decreasing = TRUE)
ns_corpus <- tm_map(corpus, removeWords, stopwords('english'))
ns_corpus
dtm <- DocumentTermMatrix(ns_corpus)
dtm
class(dtm)
sapply(dtm, class)
findFreqTerms(dtm,20)
findAssocs(dtm, "wrong", 0.2)
dtm <- as.matrix(dtm)
dtm[1:10, 1:20]
tdm <- TermDocumentMatrix(ns_corpus)
tdm<- as.matrix(tdm)
tdm[1:100, 1:20]
# Bar Plot
w <- rowSums(tdm)
w <- subset(w, w>=200)
barplot(w, las = 2, col = rainbow(50))
library(wordcloud)
w <- sort(rowSums(tdm),decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c())
library(wordcloud)
install.packages(wordcloud)
install.packages("wordcloud")
library(wordcloud)
w <- sort(rowSums(tdm),decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c())
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(7,0.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,1))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(10,1))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(1,1))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(4,.2))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(4,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(10,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(7,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(7,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(7,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(7,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(7,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(7,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(7,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,.3))
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 200,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,.3))
wordcloud(words = names(w),
freq = w,
max.words =150,
random.order = F,
min.freq = 150,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,.3))
wordcloud(words = names(w),
freq = w,
max.words =150,
random.order = F,
min.freq = 150,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,.3))
write.csv(corpus,"DC cleaned stop.csv")
data.frame(corpus)
inspect(corpus[1:5])
corpus <- tm_map(corpus,content_transformer(remove_newline))
remove_newline <- function(x) gsub('\n','',x)
corpus <- tm_map(corpus,content_transformer(remove_newline))
inspect(corpus[1:5])
corpus <- tm_map(corpus, stripWhitespace)
inspect(corpus[1:5])
write.csv(corpus,"DC cleaned stop.csv")
writeLines(as.character(corpus),"DC cleaned stop.txt")
getwd()
setwd("C:/Users/hzeig/Documents/Data Science/NYCDSA/projects/Reddit")
writeLines(as.character(corpus),"DC cleaned stop.txt")
writeLines(as.character(ns_corpus),"DC cleaned nostop.txt")
dc_stop <- str(readLines("DC cleaned stop"))
getwd()
dc_stop <- str(readLines("DC cleaned stop.txt"))
dc_stop <- readLines("DC cleaned stop.txt")
dc_nostop <- readLines("DC cleaned nostop.txt")
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_stop)
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_stop)
```
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_stop)
head(s)
get_nrc_sentiment('ugly')
get_nrc_sentiment('transphobic')
get_nrc_sentiment('homophobic')
get_nrc_sentiment('bootlickers')
head(s)
# Bar plot
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores')
# Bar plot
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores')
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_nostop)
head(s)
get_nrc_sentiment('bootlickers')
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_nostop)
head(s)
get_nrc_sentiment('bootlickers')
# Bar plot
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores')
# Bar plot
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores')
# Read file
dc_stop <- readLines("DC cleaned stop.txt")
dc_nostop <- readLines("DC cleaned nostop.txt")
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_stop)
n <- get_nrc_sentiment(dc_nostop)
head(s)
get_nrc_sentiment('bootlickers')
# Bar plot
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores')
# Read file
dc_stop <- readLines("DC cleaned stop.txt")
dc_nostop <- readLines("DC cleaned nostop.txt")
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_stop)
n <- get_nrc_sentiment(dc_nostop)
head(s)
get_nrc_sentiment('bootlickers')
# Bar plot
barplot(colSums(s,n),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores')
class(s)
head(s)
df <- rbind(s,n)
# Bar plot
barplot(colSums(df),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores')
# Bar plot
barplot(colSums(df),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores')
get_nrc_sentiment('worst')
get_nrc_sentiment('the worst')
get_nrc_sentiment("fucked")
get_nrc_sentiment("fucked up")
get_nrc_sentiment("trash") # 'bootlickers', 'worst"
get_nrc_sentiment("terrible") # 'bootlickers', 'worst"
install.packages("pacman")
get_nrc_sentiment("transphobic") # 'bootlickers', 'worst', 'fucked'
get_nrc_sentiment("jealous") # 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic',
get_nrc_sentiment("racist") # 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic',
get_nrc_sentiment("asshole") # 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic',
get_nrc_sentiment("dick") # 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic',
get_nrc_sentiment("jerk") # 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic',
get_nrc_sentiment("worse") # 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic', 'dick',
get_nrc_sentiment("ignorant") # 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic', 'dick',
View(s)
writeLines(ns_corpus,"DC cleaned nostop.txt")
library(sentimentr)
# sentimentr
s <- average_downweighted_zero(s)
s
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_stop)
# Read file
dc_stop <- readLines("DC cleaned stop.txt")
dc_nostop <- readLines("DC cleaned nostop.txt") # 3000+ less elements, why
# NRC Sentiment scores
s <- get_nrc_sentiment(dc_stop)
s
# sentimentr
dwz <- average_downweighted_zero(dc_stop)
class(dc_stop)
# sentimentr
dwz <- average_downweighted_zero(dc_stop, na.rm = True)
# sentimentr
dwz <- average_downweighted_zero(dc_stop, na.rm = T)
# sentimentr
dwz <- average_downweighted_zero(s)
wms <- average_weighted_mixed_sentiment(dwz)
dwz
wms
# Bar plot
barplot(colSums(),
las = 2,
col = ,
ylab = 'Count',
main = 'Sentiment Scores')
# Bar plot
barplot(colSums(n),
las = 2,
col = ,
ylab = 'Count',
main = 'Sentiment Scores')
# Bar plot
barplot(colSums(s),
las = 2,
col = ,
ylab = 'Count',
main = 'Sentiment Scores')
View(dcfull)
dc_urls <- find_thread_urls(keywords="dave chapelle netflix trans", sort_by = 'relevance', period = 'month')
library(RedditExtractoR)
dc_urls <- find_thread_urls(keywords="dave chapelle netflix trans", sort_by = 'relevance', period = 'month')
View(dc_urls)
unique(dc_urls$subreddit)
jm_urls <- find_thread_urls(keywords="joe manchin climate bill", sort_by = 'relevance', period = 'month')
unique(jm_urls$subreddit)
get_nrc_sentiment("fucked") # colorful words that have no sentiment value: 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic', 'dick',
get_nrc_sentiment("transphobic") # colorful words that have no sentiment value: 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic', 'dick',
get_nrc_sentiment("terrible") # colorful words that have no sentiment value: 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic', 'dick',
get_nrc_sentiment("worst") # colorful words that have no sentiment value: 'bootlickers', 'worst', 'fucked', 'transphobic', 'homophobic', 'dick',
# sentimentr
dwz <- average_downweighted_zero(s)
wms <- average_weighted_mixed_sentiment(dwz)
wms
dwz
wms <- average_weighted_mixed_sentiment(s)
wms
# sentimentr
dwz <- average_downweighted_zero(dc_stop)
View(dc_urls)
View(dcfull)
library(dplyr)
read.table(file = '3DS.tsv', sep = ',', header = F)
read.table(file = 'subreddit/3DS.tsv', sep = ',', header = F)
read.table(file = 'subreddits/3DS.tsv', sep = ',', header = F)
read.table(file = 'subreddits/3DS.tsv', sep = '\t', header = F)
subreddit_lex <- read.table(file = 'subreddits/3DS.tsv', sep = '\t', header = F)
dcfull %>%  group_by(url) %>% contains(subreddit_lex)
subreddit_lex
class(subreddit_lex)
dcfull %>%  group_by(url) %>% contains(subreddit_lex$V1)
class(subreddit_lex$V1)
dcfull %>%  group_by(url) %>% contains(comment, subreddit_lex$V1)
class(dcfull$comment)
contains(dc_full$comment, subreddit_lex$V1)
contains(dcfull$comment, subreddit_lex$V1)
is.element(dcfull$comment, subreddit_lex$V1)
View(df)
View(corpus)
class(corpus)
cleanedtext <- tm_map(corpus, removeWords, c('dave', 'chapelle', 'netflix', 'trans', 'walkout', 'chappelle','just','like'))
class(cleanedtext)
class(convert.tm.to.character(corpus))
install.packages("textreg")
install.packages(rtools)
install.packages("Rtools")
library(textreg)
class(convert.tm.to.character(corpus))
convert.tm.to.character(corpus)
class(corpus)
?convert.tm.to.character
sample <- dcfull %>% group_by(url) %>% sample_n(10)
sample <- dcfull %>% group_by(url) %>% nrow()
sample
") %>% nrow()
sample
sample <- dcfull %>% filter(url = "https://www.reddit.com/r/SocialJusticeInAction/comments/qczafo/demands_from_the_netflix_trans_team_after_the/") %>% nrow()
sample
sample <- dcfull %>% filter(url = "https://www.reddit.com/r/SocialJusticeInAction/comments/qczafo/demands_from_the_netflix_trans_team_after_the/")
sample
sample <- dcfull %>% filter(url == "https://www.reddit.com/r/SocialJusticeInAction/comments/qczafo/demands_from_the_netflix_trans_team_after_the/")
sample
sample <- dcfull %>% filter(url == "https://www.reddit.com/r/SocialJusticeInAction/comments/qczafo/demands_from_the_netflix_trans_team_after_the/") %>% nrow()
sample
sample <- dcfull %>% filter(url == "https://www.reddit.com/r/SocialJusticeInAction/comments/qczafo/demands_from_the_netflix_trans_team_after_the/")
sample <- dcfull %>% filter(url == "https://www.reddit.com/r/SocialJusticeInAction/comments/qczafo/demands_from_the_netflix_trans_team_after_the/")
is.element(sample$comment, subreddit_lex$V1)
is.element(sample$comment[1], subreddit_lex$V1)
class(subreddit_lex$V1)
View(subreddit_lex)
dcfull$comment[1]
length(grep(subreddit_lex$V1[1],sample$comment))
length(grep(subreddit_lex$V1[2],sample$comment))
length(grep(subreddit_lex$V1[3],sample$comment))
length(grep(subreddit_lex$V1[4],sample$comment))
length(grep(subreddit_lex$V1[5],sample$comment))
sapply(length(grep(subreddit_lex$V1[2])),sample$comment)
sapply(sample$comment,length(grep(subreddit_lex$V1[2])))
sapply(sample$comment,length(grep(subreddit_lex$V1[2],)))
sapply(sample$comment,length(grep(subreddit_lex$V1[2],x)))
sapply(subreddit_lex$V1,length(grep(subreddit_lex$V1,sample$comment)))
library(syuzhet)
library(sentimentr)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(vader)
dcfull <- read.csv("Dave Chapelle Comments.csv")
dc_sents <- vader_df(dcfull$comment)
dc_sents <- vader_df(dcfull$comment)
```
View(dc_sents)
sum(dc_sents$compound)
dc_sents <- dc_sents %>% drop_na()
library(dplyr)
dc_sents <- dc_sents %>% drop_na()
library(tidyverse)
dc_sents <- dc_sents %>% drop_na()
sum(dc_sents$compound)
sum(dc_sents$neg)
sum(dc_sents$pos - dc_sents$neg)
sum(dc_sents$neg - dc_sents$pos)
plot(sum(dc_sents$neg), sum(dc_sents$pos), sum(dc_sents$neu))
write.csv(dc_sents,"DC vader sents.csv", row.names = F)
drops <- c("word_scores","but_count")
dc_sents[ , !(names(dc_sents) %in% drops)]
dc_sents1 <- dc_sents[ , !(names(dc_sents) %in% drops)]
pivot_longer(dc_sents1, names_to = "type", values_to = "valence", values_drop_na = TRUE)
pivot_longer(dc_sents1, !text, names_to = "type", values_to = "valence", values_drop_na = TRUE)
drops <- c('text',"word_scores","but_count")
dc_sents1 <- dc_sents[ , !(names(dc_sents) %in% drops)]
pivot_longer(dc_sents1, ., names_to = "type", values_to = "valence", values_drop_na = TRUE)
pivot_longer(dc_sents1, names_to = "type", values_to = "valence", values_drop_na = TRUE)
drops <- c("word_scores","but_count")
dc_sents1 <- dc_sents[ , !(names(dc_sents) %in% drops)]
pivot_longer(dc_sents1, !text, names_to = "type", values_to = "valence", values_drop_na = TRUE)
dsents1 <- pivot_longer(dc_sents1, !text, names_to = "type", values_to = "valence", values_drop_na = TRUE)
barplot(dc_sents1$type)
dsents1 <- pivot_longer(dc_sents1, !text, names_to = "type", values_to = "valence", values_drop_na = TRUE)
barplot(dc_sents1$type, dc_sents1$valence)
?barplot
barplot(dc_sents1$valence, dc_sents1$type)
dsents1 %>% group_by(type)
dsents1 %>% group_by(type) %>% sum()
dsents1 %>% group_by(type) %>% sum(valence)
dsents1 %>% aggregate(valence, by=list(type), FUN=sum)
dsents1 %>% aggregate('valence', by=list('type'), FUN=sum)
aggregate(dsents1$valence, by=list(type=dsents1$type), FUN=sum)
dsents1<- aggregate(dsents1$valence, by=list(type=dsents1$type), FUN=sum)
barplot(dc_sents1$valence, dc_sents1$type)
barplot(dc_sents1$valenc)
barplot(dsents1$valenc)
barplot(dsents1$valence)
rm(list=ls())
